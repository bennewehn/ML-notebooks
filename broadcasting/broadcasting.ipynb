{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b80da8f",
   "metadata": {},
   "source": [
    "## Implement Broadcasting\n",
    "\n",
    "1. flat index -> multidimensional index\n",
    "2. set strides of broadcasting dimensions to 0\n",
    "2. multidimensional index -> flat index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bad2004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "from operator import mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ba426f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbElements = lambda shape: reduce(mul, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "75d5f5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 4, 1)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getStrides(shape: tuple) -> tuple:\n",
    "    #build strides \n",
    "    strides = [1]\n",
    "    for i in reversed(shape[1:]):\n",
    "       strides.append(strides[-1] * i)\n",
    "    strides.reverse()\n",
    "    return tuple(strides)\n",
    "\n",
    "getStrides((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e29f8ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiToFlat(shape: tuple, idxs: tuple, strides: tuple) -> int:\n",
    "    # check if shape is compatible to array\n",
    "    assert len(idxs) == len(shape)\n",
    "    assert len(strides) == len(shape)\n",
    "\n",
    "    for i in range(len(shape)):\n",
    "        # A stride of 0 means broadcasting that dimension\n",
    "        # The index can be larger than the shape's dimension size (which will be 1).\n",
    "        # Only check bounds for non-broadcasted dimensions:\n",
    "        if strides[i] != 0:\n",
    "            assert idxs[i] < shape[i], f\"Index {idxs[i]} out of bounds for shape {shape[i]} at dimension {i}\"\n",
    "        assert idxs[i] >= 0, \"No negative index allowed\"\n",
    "\n",
    "    return np.dot(strides, idxs).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1d9c0246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiToFlat((2, 2, 3), (0, 0, 1), getStrides((2, 2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1dbb0835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(8)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (1, 2, 3)\n",
    "b = (2, 3, 0)\n",
    "np.dot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2cd34a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatToMulti(index: int, shape: tuple) -> tuple:\n",
    "    assert index < reduce(lambda x, y: x*y, shape), \"Index out of bounds\"\n",
    "    assert index >= 0, \"No negative index allowed\"\n",
    "    \n",
    "    strides = getStrides(shape)\n",
    "\n",
    "    res = []\n",
    "    for i in strides:\n",
    "        res.append(index // i)\n",
    "        index %= i\n",
    "    return tuple(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a01b2487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatToMulti(0, (2, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "76ba8321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 2)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def outputShape(a: tuple, b: tuple) -> tuple:\n",
    "    smaller, larger = (b, a) if len(a) > len(b) else (a, b)\n",
    "    smaller = (1,) * abs(len(a)-len(b)) + smaller\n",
    "    res = []\n",
    "    for x, y in zip(smaller, larger):\n",
    "        if x == y or x == 1 or y == 1:\n",
    "            res += [max(x,y)]\n",
    "        else:\n",
    "            raise RuntimeError(\"shapes are not compatible.\")\n",
    "    return tuple(res)\n",
    "\n",
    "outputShape((3, 2), (1, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2ae20e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (2, 1, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 2.,  4.,  6.,  8., 10.]],\n",
       "\n",
       "       [[ 7.,  9., 11., 13., 15.]]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def AddOperator(a: np.ndarray, b: np.ndarray):\n",
    "    shapeA = a.shape\n",
    "    shapeB = b.shape\n",
    "    a = a.flatten()\n",
    "    b = b.flatten()\n",
    "\n",
    "    outShape = outputShape(shapeA, shapeB)\n",
    "    print(\"Output shape:\", outShape)\n",
    "    out = np.zeros(outShape).flatten()\n",
    "\n",
    "    out_ndim = len(outShape)\n",
    "\n",
    "    padded_shapeA = (1,) * (out_ndim - len(shapeA)) + shapeA\n",
    "    padded_shapeB = (1,) * (out_ndim - len(shapeB)) + shapeB\n",
    "\n",
    "    stridesA = getStrides(padded_shapeA) \n",
    "    stridesB = getStrides(padded_shapeB)\n",
    "\n",
    "    bcast_stridesA = tuple((0 if x == 1 else y for x, y in zip(padded_shapeA, stridesA)))\n",
    "    bcast_stridesB = tuple((0 if x == 1 else y for x, y in zip(padded_shapeB, stridesB)))\n",
    "\n",
    "    for i in range(numbElements(outShape)):\n",
    "        multi_idx = flatToMulti(i, outShape)\n",
    "        idxA = multiToFlat(padded_shapeA, multi_idx, bcast_stridesA)\n",
    "        idxB = multiToFlat(padded_shapeB, multi_idx, bcast_stridesB)\n",
    "        out[i] = a[idxA] + b[idxB]\n",
    "\n",
    "    \n",
    "    return out.reshape(outShape)\n",
    "        \n",
    "a = np.linspace(1, 10, 10).reshape(2, 1, 5)\n",
    "b= np.linspace(1, 5, 5).reshape(1, 5)\n",
    "AddOperator(a, b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0db04f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 3, 3)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# broadcasting matmul\n",
    "# 1. split in batch_dims and matrix_dims (m, n)\n",
    "# 2. check matrix multiplication compatibility (k, n) x (n, m) -> (k, m)\n",
    "# 3. check batch_dims with broadcasting rules\n",
    "# 4. output shape is (broadcasted_batch_dims, k, m)\n",
    "# 5. perform matrix multiplication with the broadcasted batch dimensions\n",
    "a =np.ones((1, 3, 2))\n",
    "b =np.ones((2, 4, 2, 3))\n",
    "out = a @ b\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ab908e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 3, 3)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getMatmulOutputShape(shapeA: tuple, shapeB: tuple) -> tuple:\n",
    "    # 1. split in batch_dims and matrix_dims (m, n)\n",
    "    assert len(shapeA) >= 2 and len(shapeB) >= 2\n",
    "    matrix_dims_A = shapeA[-2:]\n",
    "    matrix_dims_B = shapeB[-2:]\n",
    "    batch_dims_A = shapeA[:-2]\n",
    "    batch_dims_B = shapeB[:-2]\n",
    "    # 2. check matrix multiplication compatibility (k, n) x (n, m) -> (k, m)\n",
    "    assert matrix_dims_A[1] == matrix_dims_B[0], f\"matrix-multiplication shape mismatch: {matrix_dims_A} x {matrix_dims_B}\"\n",
    "    # 3. check batch_dims with broadcasting rules\n",
    "    batch_dims_brcast = outputShape(batch_dims_A, batch_dims_B)\n",
    "    # 4. output shape is (broadcasted_batch_dims, k, m)\n",
    "    out_shape = batch_dims_brcast + (matrix_dims_A[0], matrix_dims_B[1])\n",
    "    return out_shape\n",
    "\n",
    "getMatmulOutputShape((1, 3, 2), (2, 4, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ffa2b83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5., 5., 5.],\n",
       "       [5., 5., 5.],\n",
       "       [5., 5., 5.],\n",
       "       [5., 5., 5.]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_shape = (4,5)\n",
    "b_shape = (5,3)\n",
    "a = np.ones(a_shape)\n",
    "b = np.ones(b_shape)\n",
    "print(a)\n",
    "print(b)\n",
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ad5949af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 5., 5., 5., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# naive matmul\n",
    "a_shape = (4,5)\n",
    "b_shape = (5,3)\n",
    "a = np.ones(a_shape).flatten()\n",
    "b = np.ones(b_shape).flatten()\n",
    "out_shape = getMatmulOutputShape(a_shape, b_shape)\n",
    "out = np.zeros(numbElements(out_shape)).flatten()\n",
    "\n",
    "# dot product first row of A with first col of B\n",
    "for row in range(a_shape[1]):\n",
    "    a_idx = multiToFlat(a_shape, (0, row), getStrides(a_shape)) \n",
    "    b_idx = multiToFlat(b_shape, (row, 0), getStrides(b_shape)) \n",
    "    out[0] += a[a_idx] * b[b_idx]\n",
    "\n",
    "# dot product first row of A with second col of B\n",
    "for row in range(a_shape[1]):\n",
    "    a_idx = multiToFlat(a_shape, (0, row), getStrides(a_shape)) \n",
    "    b_idx = multiToFlat(b_shape, (row, 1), getStrides(b_shape)) \n",
    "    out[1] += a[a_idx] * b[b_idx]\n",
    "\n",
    "# dot product first row of A with third col of B\n",
    "for row in range(a_shape[1]):\n",
    "    a_idx = multiToFlat(a_shape, (0, row), getStrides(a_shape)) \n",
    "    b_idx = multiToFlat(b_shape, (row, 2), getStrides(b_shape)) \n",
    "    out[2] += a[a_idx] * b[b_idx]\n",
    "\n",
    "# dot product second row of A with first col of B\n",
    "for row in range(a_shape[1]):\n",
    "    a_idx = multiToFlat(a_shape, (1, row), getStrides(a_shape)) \n",
    "    b_idx = multiToFlat(b_shape, (row, 0), getStrides(b_shape)) \n",
    "    out[3] += a[a_idx] * b[b_idx]\n",
    "\n",
    "# ...\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "42d52cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3., 3.],\n",
       "       [3., 3., 3.],\n",
       "       [3., 3., 3.],\n",
       "       [3., 3., 3.]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# naive matmul\n",
    "a = np.ones((4, 3))\n",
    "b = np.ones((3, 3))\n",
    "\n",
    "def naiveMatmul(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    a_shape = a.shape\n",
    "    b_shape = b.shape\n",
    "    out_shape = getMatmulOutputShape(a_shape, b_shape)\n",
    "    a = a.flatten()\n",
    "    b = b.flatten()\n",
    "    out = np.zeros(numbElements(out_shape))\n",
    "\n",
    "    for i in range(a_shape[0]):\n",
    "        for j in range(b_shape[1]):\n",
    "            for k in range(a_shape[1]):\n",
    "                a_idx = multiToFlat(a_shape, (i, k), getStrides(a_shape)) \n",
    "                b_idx = multiToFlat(b_shape, (k, j), getStrides(b_shape))\n",
    "                out_idx = multiToFlat(out_shape, (i, j), getStrides(out_shape))\n",
    "                out[out_idx] += a[a_idx] * b[b_idx]\n",
    "    \n",
    "    return out.reshape(out_shape)\n",
    "\n",
    "naiveMatmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "5a330f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveMatmul(a: np.ndarray, b: np.ndarray, out: np.ndarray):\n",
    "    a_shape = a.shape\n",
    "    b_shape = b.shape\n",
    "    out_shape = getMatmulOutputShape(a_shape, b_shape)\n",
    "    a = a.flatten()\n",
    "    b = b.flatten()\n",
    "\n",
    "    for i in range(a_shape[0]):\n",
    "        for j in range(b_shape[1]):\n",
    "            for k in range(a_shape[1]):\n",
    "                a_idx = multiToFlat(a_shape, (i, k), getStrides(a_shape)) \n",
    "                b_idx = multiToFlat(b_shape, (k, j), getStrides(b_shape))\n",
    "                out_idx = multiToFlat(out_shape, (i, j), getStrides(out_shape))\n",
    "                out[out_idx] += a[a_idx] * b[b_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ba082375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, (3,))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getTensor(shape: tuple, idx: tuple, tensor_strides: tuple) -> tuple[int, tuple]: \n",
    "   assert len(idx) <= len(shape), f\"Too many indices for tensor with shape {shape}\"\n",
    "   assert len(tensor_strides) == len(shape)\n",
    "   for i in range(len(idx)):\n",
    "      if tensor_strides[i] != 0:  # Broadcasting is enabled for that dimension\n",
    "         assert idx[i] < shape[i], f\"Index {idx[i]} is out of bounds for axis {i} with size {shape[i]}\"\n",
    "      assert 0 <= idx[i]\n",
    "\n",
    "   # calculate offset\n",
    "   idx_padd = idx + (0, ) * (len(shape) - len(idx))\n",
    "   offset = np.dot(idx_padd, tensor_strides) \n",
    "   # new shape\n",
    "   new_shape = shape[len(idx):]\n",
    "\n",
    "   return (offset.item(), new_shape)\n",
    "\n",
    " \n",
    "t = np.zeros((3, 2, 2, 3))\n",
    "getTensor(t.shape, (2,0, 0), getStrides(t.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "4e7f351e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 22.,  28.],\n",
       "        [ 49.,  64.],\n",
       "        [ 76., 100.],\n",
       "        [103., 136.]],\n",
       "\n",
       "       [[130., 172.],\n",
       "        [157., 208.],\n",
       "        [184., 244.],\n",
       "        [211., 280.]]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MatmulOperator(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    out_shape = getMatmulOutputShape(a.shape,  b.shape)\n",
    "    # naive batch matmul\n",
    "    out = np.zeros(numbElements(out_shape))\n",
    "    batch_dims = out_shape[:-2]\n",
    "\n",
    "    padded_shapeA = (1,) * (len(out_shape) - len(a.shape)) + a.shape\n",
    "    padded_shapeB = (1,) * (len(out_shape) - len(b.shape)) + b.shape\n",
    "\n",
    "    stridesA = getStrides(padded_shapeA) \n",
    "    stridesB = getStrides(padded_shapeB)\n",
    "\n",
    "    bcast_stridesA = tuple((0 if x == 1 else y for x, y in zip(padded_shapeA, stridesA)))\n",
    "    bcast_stridesB = tuple((0 if x == 1 else y for x, y in zip(padded_shapeB, stridesB)))\n",
    "\n",
    "    a = a.flatten()\n",
    "    b = b.flatten()\n",
    "\n",
    "    stridesOut = getStrides(out_shape)\n",
    "    # flat batch iteration\n",
    "    for i in range(numbElements(batch_dims)):\n",
    "        batch_idx = flatToMulti(i, batch_dims)\n",
    "\n",
    "        matrixA_offset, matrixA_shape = getTensor(padded_shapeA, batch_idx, bcast_stridesA)\n",
    "        matrixB_offset, matrixB_shape = getTensor(padded_shapeB, batch_idx, bcast_stridesB)\n",
    "\n",
    "        matA = a[matrixA_offset : matrixA_offset + numbElements(matrixA_shape)]\n",
    "        matB = b[matrixB_offset : matrixB_offset + numbElements(matrixB_shape)]\n",
    "        \n",
    "        out_ref_offset, out_ref_shape = getTensor(out_shape, batch_idx, stridesOut)\n",
    "        out_ref = out[out_ref_offset : out_ref_offset + numbElements(out_ref_shape)]\n",
    "        \n",
    "        naiveMatmul(matA.reshape(matrixA_shape), matB.reshape(matrixB_shape), out_ref) # only reshape to conveniently pass the shape\n",
    "    \n",
    "    return out.reshape(out_shape)        \n",
    "\n",
    "shapeA = (2, 4, 3)\n",
    "shapeB = (3, 2)\n",
    "A = np.arange(1, np.prod(shapeA) + 1).reshape(shapeA) \n",
    "B = np.arange(1, np.prod(shapeB) + 1).reshape(shapeB) \n",
    "MatmulOperator(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "50d1a028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Own implementation: 7.035489 seconds\n",
      "Numpy implementation: 0.000707 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "shapeA = (1, 10, 200)\n",
    "shapeB = (5, 1, 200, 100)\n",
    "A = np.arange(1, np.prod(shapeA) + 1).reshape(shapeA) \n",
    "B = np.arange(1, np.prod(shapeB) + 1).reshape(shapeB) \n",
    "\n",
    "start = time.perf_counter()\n",
    "MatmulOperator(A, B)\n",
    "end = time.perf_counter()\n",
    "print(f\"Own implementation: {end - start:.6f} seconds\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "res = A @ B\n",
    "end = time.perf_counter()\n",
    "print(f\"Numpy implementation: {end - start:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
